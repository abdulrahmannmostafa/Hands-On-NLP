{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.4119e+08, 1.7012e-42])\n"
     ]
    }
   ],
   "source": [
    "# tensor filled with uninitialized data\n",
    "em = torch.empty(2)\n",
    "print(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.4842e-44, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "em = torch.empty(1, 2)\n",
    "print(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.5000, 0.1000])\n"
     ]
    }
   ],
   "source": [
    "# Initializing Tensor Directly from data\n",
    "x = torch.tensor([2.5, 0.1])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3215, 0.4743, 0.9871],\n",
      "        [0.9320, 0.1578, 0.1746],\n",
      "        [0.3255, 0.2243, 0.5571],\n",
      "        [0.1954, 0.1402, 0.0890],\n",
      "        [0.8013, 0.9231, 0.3468]])\n",
      "tensor([0.3215, 0.9320, 0.3255, 0.1954, 0.8013])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "print(x[:, 0])\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 58,  64],\n",
       "        [139, 154]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b = torch.tensor([[7, 8], [9, 10], [11, 12]])\n",
    "c = torch.matmul(a, b)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statistical operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.min(\n",
      "values=tensor([1., 3.]),\n",
      "indices=tensor([0, 0])) tensor(4.) tensor(2.5000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tensor = torch.Tensor([[1, 2],\n",
    "                       [3, 4]])\n",
    "\n",
    "# get minmum value in each row\n",
    "tensor_min = torch.min(tensor, dim=1)\n",
    "# get max value in tensor\n",
    "tensor_max = torch.max(tensor)\n",
    "# calculate mean value\n",
    "tensor_mean = torch.mean(tensor)\n",
    "...  # Every single mathematical function you could imagine.\n",
    "\n",
    "print(tensor_min, tensor_max, tensor_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 4.]) tensor([1.5000, 3.5000, 5.5000])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.Tensor([[1, 2],\n",
    "                       [3, 4],\n",
    "                       [5, 6]])  # shape = (3, 2)\n",
    "\n",
    "tensor_mean_row = torch.mean(tensor, dim=0)  # shape = (2,) Averaging over 1st dimension (along columns)\n",
    "tensor_mean_col = torch.mean(tensor, dim=1)  # shape = (3,) Averaging over 2nd dimension (along rows)\n",
    "\n",
    "print(tensor_mean_row, tensor_mean_col) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!![image.png](https://i.stack.imgur.com/ORqaP.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6015, 0.3014, 0.6270, 0.6938],\n",
      "        [0.9450, 0.6325, 0.8312, 0.3869],\n",
      "        [0.5598, 0.1211, 0.6853, 0.1790],\n",
      "        [0.4458, 0.8079, 0.1742, 0.2735]])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4,4)\n",
    "print(x)\n",
    "# view function is a view of the tensor, it does not create a new tensor\n",
    "# but modifies the original tensor and changes the shape\n",
    "y = x.view(-1,4 )\n",
    "\n",
    "z = x.view(1,16 )\n",
    "print(y.size())\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "type of b <class 'numpy.ndarray'>\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "type of c <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b= a.numpy()\n",
    "print(b)\n",
    "print(f\"type of b {type(b)}\")\n",
    "c = torch.from_numpy(b)\n",
    "print(c)\n",
    "print(f\"type of c {type(c)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    x = torch.ones(5, device=device)\n",
    "    y = torch.ones(5)\n",
    "    # By default, tensors are created on the CPU. We need to explicitly move tensors \n",
    "    # to the GPU using .to method (after checking for GPU availability)\n",
    "    y = y.to(device)\n",
    "    z = x*y\n",
    "    z=z.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The general framework for training your model is as follows\n",
    "for epoch in range(num_epochs):\n",
    "\tfor i in range(num_batches):\n",
    "\t\tForward pass\n",
    "\t\tLoss calculation\n",
    "\t\tBackward pass\n",
    "\t\tWeights update\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines arrays in a vertically stacked sequence (used for data manipulation)\n",
    "from numpy import vstack\n",
    "\n",
    "# Reads a CSV file into a DataFrame (used for loading datasets)\n",
    "from pandas import read_csv\n",
    "\n",
    "# Encodes categorical labels into numerical format (used for label preprocessing)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Calculates the accuracy of a classification model (used for model evaluation)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Defines a custom dataset class for PyTorch (used for handling data)\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Creates a DataLoader for efficient batch processing in PyTorch (used for data loading)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Splits a dataset into training and validation sets (used for data splitting)\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Represents a multi-dimensional matrix in PyTorch (used for tensor manipulation)\n",
    "from torch import Tensor\n",
    "\n",
    "# Implements a linear layer in a neural network (used for defining neural network architecture)\n",
    "from torch.nn import Linear\n",
    "\n",
    "# Applies rectified linear unit (ReLU) activation function (used for introducing non-linearity)\n",
    "from torch.nn import ReLU\n",
    "\n",
    "# Applies sigmoid activation function (used for binary classification output)\n",
    "from torch.nn import Sigmoid\n",
    "\n",
    "# Base class for all neural network modules in PyTorch (used for creating custom models)\n",
    "from torch.nn import Module\n",
    "\n",
    "# Stochastic Gradient Descent optimizer (used for model optimization during training)\n",
    "from torch.optim import SGD\n",
    "\n",
    "# Binary Cross Entropy Loss function (used for binary classification problems)\n",
    "from torch.nn import BCELoss\n",
    "\n",
    "# Initializes weights using Kaiming uniform initialization (used for weight initialization)\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "\n",
    "# Initializes weights using Xavier (Glorot) uniform initialization (used for weight initialization)\n",
    "from torch.nn.init import xavier_uniform_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "PyTorch provides two data primitives: ``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset``\n",
    "that allow you to use pre-loaded datasets as well as your own data.\n",
    "``Dataset`` stores the samples and their corresponding labels, and ``DataLoader`` wraps an iterable around\n",
    "the ``Dataset`` to enable easy access to the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset definition\n",
    "# A custom Dataset class must implement three functions: __init__, __len__, and __getitem__.\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    # The __init__ function is run once when instantiating the Dataset object\n",
    "    def __init__(self, path):\n",
    "        # load the csv file as a dataframe\n",
    "        df = read_csv(path, header=None)\n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[:, :-1]\n",
    "        original_labels = df.values[:, -1]\n",
    "        # ensure input data is floats\n",
    "        self.X = self.X.astype(\"float32\")\n",
    "        # label encode target and ensure the values are floats\n",
    "        self.y = LabelEncoder().fit_transform(original_labels)\n",
    "\n",
    "        # this dictionary\n",
    "        self.encoding_mapping = {\n",
    "            encoded_label: original_label\n",
    "            for encoded_label, original_label in zip(self.y, original_labels)\n",
    "        }\n",
    "\n",
    "        # ensure the target is float\n",
    "        self.y = self.y.astype(\"float32\")\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    # The __len__ function returns the number of samples in our dataset.\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    # The __getitem__ function loads and returns a sample from the dataset at the given index idx\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "\n",
    "# prepare the dataset\n",
    "def prepare_data(path):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    # The Dataset retrieves our dataset’s features and labels one sample at a time.\n",
    "    # While training a model, we typically want to pass samples in “minibatches”,\n",
    "    # reshuffle the data at every epoch to reduce model overfitting,\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return dataset.encoding_mapping, train_dl, test_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 10)\n",
    "        # initializes the weights of the self.hidden1 layer using the Kaiming (He) Uniform initialization method\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity=\"relu\")\n",
    "        self.act1 = ReLU()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(10, 8)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity=\"relu\")\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(8, 1)\n",
    "        # initializes the weights of the self.hidden3 layer using the Xavier (Glorot) Uniform initialization method.\n",
    "        # it is commonly used for layers with hyperbolic tangent (tanh) or sigmoid activation functions.\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        self.act3 = Sigmoid()\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = BCELoss()  # Binary Cross-Entropy\n",
    "    # Stochastic Gradient Descent Optimizer\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(100):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # compute the model output Forward Pass\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    with torch.no_grad():\n",
    "        predictions, actuals = list(), list()\n",
    "        for i, (inputs, targets) in enumerate(test_dl):\n",
    "            # evaluate the model on the test set\n",
    "            yhat = model(inputs)\n",
    "            # retrieve numpy array\n",
    "            yhat = yhat.detach().numpy()\n",
    "            actual = targets.numpy()\n",
    "            actual = actual.reshape((len(actual), 1))\n",
    "            # round to class values\n",
    "            yhat = yhat.round()\n",
    "            # store\n",
    "            predictions.append(yhat)\n",
    "            actuals.append(actual)\n",
    "        predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "        # calculate accuracy\n",
    "        acc = accuracy_score(actuals, predictions)\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Predictioin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a class prediction for one row of data\n",
    "def predict(row, model: Module):\n",
    "    # convert row to data\n",
    "    model.eval()  # This is equivalent with self.train(False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        row = Tensor([row])\n",
    "        # make prediction\n",
    "        yhat = model(row)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training (main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000002DBB479FD70>\n",
      "235 116\n",
      "Accuracy: 0.879\n",
      "Predicted: 0.995 (class=1) which is (class=g)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bedom\\AppData\\Local\\Temp\\ipykernel_3824\\2420286245.py:59: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  % (yhat, yhat.round(), encoding_mapping[int(yhat.round())])\n",
      "C:\\Users\\bedom\\AppData\\Local\\Temp\\ipykernel_3824\\2420286245.py:58: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  \"Predicted: %.3f (class=%d) which is (class=%s)\"\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "# dataset link https://datahub.io/machine-learning/ionosphere\n",
    "path = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv\"\n",
    "encoding_mapping, train_dl, test_dl = prepare_data(path)\n",
    "print(train_dl)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "# define the network\n",
    "model = MLP(34)\n",
    "\n",
    "model.train(True)  # set it to False for inference\n",
    "\n",
    "\n",
    "# train the model\n",
    "train_model(train_dl, model)\n",
    "# evaluate the model\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print(\"Accuracy: %.3f\" % acc)\n",
    "# make a single prediction (expect class=1)\n",
    "row = [\n",
    "    1,\n",
    "    0,\n",
    "    0.99539,\n",
    "    -0.05889,\n",
    "    0.85243,\n",
    "    0.02306,\n",
    "    0.83398,\n",
    "    -0.37708,\n",
    "    1,\n",
    "    0.03760,\n",
    "    0.85243,\n",
    "    -0.17755,\n",
    "    0.59755,\n",
    "    -0.44945,\n",
    "    0.60536,\n",
    "    -0.38223,\n",
    "    0.84356,\n",
    "    -0.38542,\n",
    "    0.58212,\n",
    "    -0.32192,\n",
    "    0.56971,\n",
    "    -0.29674,\n",
    "    0.36946,\n",
    "    -0.47357,\n",
    "    0.56811,\n",
    "    -0.51171,\n",
    "    0.41078,\n",
    "    -0.46168,\n",
    "    0.21266,\n",
    "    -0.34090,\n",
    "    0.42267,\n",
    "    -0.54487,\n",
    "    0.18641,\n",
    "    -0.45300,\n",
    "]\n",
    "yhat = predict(row, model)\n",
    "\n",
    "print(\n",
    "    \"Predicted: %.3f (class=%d) which is (class=%s)\"\n",
    "    % (yhat, yhat.round(), encoding_mapping[int(yhat.round())])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
